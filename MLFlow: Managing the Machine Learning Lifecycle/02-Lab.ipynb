{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "Load in same Airbnb data and create train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grid Search using scikit-learn\n",
    "\n",
    "We want to know which combination of hyperparameter values is the most effective. Fill in the code below to perform <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\" target=\"_blank\"> grid search using `sklearn`</a> over the 2 hyperparameters we looked at in the 02 notebook, `n_estimators` and `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Best &#39;n_estimators&#39;: 1000\n",
       "Best &#39;max_depth&#39;: 15\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# dictionary containing hyperparameter names and list of values we want to try\n",
    "parameters = {'n_estimators':[100,1000], \n",
    "              'max_depth':[10,15]}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "grid_rf_model = GridSearchCV(rf, parameters, cv=3)\n",
    "grid_rf_model.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_rf_model.best_estimator_\n",
    "for p in parameters:\n",
    "  print(\"Best '{}': {}\".format(p, best_rf.get_params()[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Best Model on MLflow\n",
    "\n",
    "Log the best model as `grid-random-forest-model`, its parameters, and its MSE metric under a run with name `RF-Grid-Search` in our new MLflow experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Inside MLflow Run with id 5cc76dc4771c4a3b97f90b717593bebc and artifact URI dbfs:/databricks/mlflow/3227357976396132/5cc76dc4771c4a3b97f90b717593bebc/artifacts\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "with mlflow.start_run(run_name= \"RF-Grid-Search\") as run:\n",
    "  # Create predictions of X_test using best model\n",
    "  predictions = best_rf.predict(X_test)\n",
    "  \n",
    "  # Log model\n",
    "  mlflow.sklearn.log_model(best_rf, \"grid-random-forest-model\")\n",
    "  \n",
    "  # Log params\n",
    "  model_params = best_rf.get_params()\n",
    "  [mlflow.log_param(p, model_params[p]) for p in parameters]\n",
    "  \n",
    "  # Create and log MSE metrics using predictions of X_test and its actual value y_test\n",
    "  mse = mean_squared_error(y_test, predictions)\n",
    "  mlflow.log_metric(\"mse\", mse)\n",
    "  \n",
    "  runID = run.info.run_uuid\n",
    "  artifactURI = mlflow.get_artifact_uri()\n",
    "  print(\"Inside MLflow Run with id {} and artifact URI {}\".format(runID, artifactURI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on the MLflow UI that the run `RF-Grid-Search` is logged has the best parameter values found by grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Saved Model\n",
    "\n",
    "Load the trained and tuned model we just saved. Check that the hyperparameters of this model matches that of the best model we found earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "model = mlflow.sklearn.load_model(artifactURI + \"/grid-random-forest-model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time permitting, continue to grid search over a wider number of parameters and automatically save the best performing parameters back to `mlflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time permitting, use the `MlflowClient` to interact programatically with your run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Cleanup\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "02-Lab",
  "notebookId": 3227357976396132
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
