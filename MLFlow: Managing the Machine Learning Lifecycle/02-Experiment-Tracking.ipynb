{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Experiments with MLflow\n",
    "\n",
    "Over the course of the machine learning life cycle, data scientists test many different models from various libraries with different hyperparameters.  Tracking these various results poses an organizational challenge.  In brief, storing experiments, results, models, supplementary artifacts, and code creates significant challenges.\n",
    "\n",
    "MLflow Tracking is one of the three main components of MLflow.  It is a logging API specific for machine learning and agnostic to libraries and environments that do the training.  It is organized around the concept of **runs**, which are executions of data science code.  Runs are aggregated into **experiments** where many runs can be a part of a given experiment and an MLflow server can host many experiments.\n",
    "\n",
    "Each run can record the following information:<br><br>\n",
    "\n",
    "- **Parameters:** Key-value pairs of input parameters such as the number of trees in a random forest model\n",
    "- **Metrics:** Evaluation metrics such as RMSE or Area Under the ROC Curve\n",
    "- **Artifacts:** Arbitrary output files in any format.  This can include images, pickled models, and data files\n",
    "- **Source:** The code that originally ran the experiment\n",
    "\n",
    "MLflow tracking also serves as a **model registry** so tracked models can easily be stored and, as necessary, deployed into production.\n",
    "\n",
    "Experiments can be tracked using libraries in Python, R, and Java as well as by using the CLI and REST calls.  This course will use Python, though the majority of MLflow functionality is also exposed in these other APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Logging and UI\n",
    "\n",
    "MLflow is an open source software project developed by Databricks available to developers regardless of which platform they are using.  Databricks hosts MLflow for you, which reduces deployment configuration and adds security benefits.  It is accessible on all Databricks workspaces in Azure and AWS.  It is not currently enabled on Community Edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databricks uses two different versions of its software.  The **Databricks Runtime** does not currently come with MLflow pre-installed.  if you are using Databricks runtime, uncomment the code below and run it to install `mlflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This installs MLflow for you only on Databricks Runtime\n",
    "# NOTE: this code does not work with ML runtime (see below)\n",
    "\n",
    "# dbutils.library.installPyPI(\"mlflow\", \"1.0.0\")\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other version of Databricks is the **Machine Learning Runtime,** which you'll notice has `ML` next to the cluster type.  **Machine Learning Runtime has `mlflow` pre-installed starting at version 5.5.**  If you have runtime **5.5 ML or later** available to you, you do not need to install `mlflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a dataset of Airbnb listings and featurize the data.  We'll use this to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigate to the MLflow UI by clicking on the `Runs` button on the top of the screen.**\n",
    "\n",
    "Every Python notebook in a Azure Databricks Workspace has its own experiment. When you use MLflow in a notebook, it records runs in the notebook experiment. A notebook experiment shares the same name and ID as its corresponding notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log a basic experiment by doing the following:<br><br>\n",
    "\n",
    "1. Start an experiment using `mlflow.start_run()` and passing it a name for the run\n",
    "2. Train your model\n",
    "3. Log the model using `mlflow.sklearn.log_model()`\n",
    "4. Log the model error using `mlflow.log_metric()`\n",
    "5. Print out the run id using `run.info.run_uuid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Thu Apr 16 07:47:54 2020 py4j imported\n",
       "/databricks/python/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
       "  &#34;10 in version 0.20 to 100 in 0.22.&#34;, FutureWarning)\n",
       "  mse: 6998.4432115548\n",
       "Inside MLflow Run with run_id aef808c7a24547e18e732577a4f03b51 and experiment_id 3227357976395900\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "with mlflow.start_run(run_name=\"Basic RF Experiment\") as run:\n",
    "  # Create model, train it, and create predictions\n",
    "  rf = RandomForestRegressor()\n",
    "  rf.fit(X_train, y_train)\n",
    "  predictions = rf.predict(X_test)\n",
    "  \n",
    "  # Log model\n",
    "  mlflow.sklearn.log_model(rf, \"random-forest-model\")\n",
    "  \n",
    "  # Create metrics\n",
    "  mse = mean_squared_error(y_test, predictions)\n",
    "  print(\"  mse: {}\".format(mse))\n",
    "  \n",
    "  # Log metrics\n",
    "  mlflow.log_metric(\"mse\", mse)\n",
    "  \n",
    "  runID = run.info.run_uuid\n",
    "  experimentID = run.info.experiment_id\n",
    "  \n",
    "  print(\"Inside MLflow Run with run_id {} and experiment_id {}\".format(runID, experimentID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the results in the UI.  Look for the following:<br><br>\n",
    "\n",
    "1. The `Experiment ID`\n",
    "2. The artifact location.  This is where the artifacts are stored in DBFS, which is backed by an Amazon S3 bucket\n",
    "3. The time the run was executed.  **Click this timestamp to see more information on the run.**\n",
    "4. The code that executed the run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking on the time of the run, take a look at the following:<br><br>\n",
    "\n",
    "1. The Run ID will match what we printed above\n",
    "2. The model that we saved, included a picked version of the model as well as the Conda environment and the `MLmodel` file, which will be discussed in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the directory structure backing this experiment.  This allows you to retrieve artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[20]: [FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/aef808c7a24547e18e732577a4f03b51/&#39;, name=&#39;aef808c7a24547e18e732577a4f03b51/&#39;, size=0)]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "artifactURL = MlflowClient().get_experiment(experimentID).artifact_location\n",
    "dbutils.fs.ls(artifactURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the contents of `random-forest-model`, which match what we see in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[21]: [FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/aef808c7a24547e18e732577a4f03b51/artifacts/random-forest-model/MLmodel&#39;, name=&#39;MLmodel&#39;, size=357),\n",
       " FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/aef808c7a24547e18e732577a4f03b51/artifacts/random-forest-model/conda.yaml&#39;, name=&#39;conda.yaml&#39;, size=130),\n",
       " FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/aef808c7a24547e18e732577a4f03b51/artifacts/random-forest-model/model.pkl&#39;, name=&#39;model.pkl&#39;, size=2532908)]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelURL = artifactURL + \"/\" + runID + \"/artifacts/random-forest-model\"\n",
    "dbutils.fs.ls(modelURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters, Metrics, and Artifacts\n",
    "\n",
    "But wait, there's more!  In the last example, you logged the run name, an evaluation metric, and your model itself as an artifact.  Now let's log parameters, multiple metrics, and other artifacts including the feature importances.\n",
    "\n",
    "First, create a function to perform this.\n",
    "\n",
    "To log artifacts, we have to save them somewhere before MLflow can log them.  This code accomplishes that by using a temporary file that it then deletes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def log_rf(experimentID, run_name, params, X_train, X_test, y_train, y_test):\n",
    "  import os\n",
    "  import matplotlib.pyplot as plt\n",
    "  import mlflow.sklearn\n",
    "  import seaborn as sns\n",
    "  from sklearn.ensemble import RandomForestRegressor\n",
    "  from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "  import tempfile\n",
    "\n",
    "  with mlflow.start_run(experiment_id=experimentID, run_name=run_name) as run:\n",
    "    # Create model, train it, and create predictions\n",
    "    rf = RandomForestRegressor(**params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rf, \"random-forest-model\")\n",
    "\n",
    "    # Log params\n",
    "    [mlflow.log_param(param, value) for param, value in params.items()]\n",
    "\n",
    "    # Create metrics\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(\"  mse: {}\".format(mse))\n",
    "    print(\"  mae: {}\".format(mae))\n",
    "    print(\"  R2: {}\".format(r2))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"mae\", mae)  \n",
    "    mlflow.log_metric(\"r2\", r2)  \n",
    "    \n",
    "    # Create feature importance\n",
    "    importance = pd.DataFrame(list(zip(df.columns, rf.feature_importances_)), \n",
    "                                columns=[\"Feature\", \"Importance\"]\n",
    "                              ).sort_values(\"Importance\", ascending=False)\n",
    "    \n",
    "    # Log importances using a temporary file\n",
    "    temp = tempfile.NamedTemporaryFile(prefix=\"feature-importance-\", suffix=\".csv\")\n",
    "    temp_name = temp.name\n",
    "    try:\n",
    "      importance.to_csv(temp_name, index=False)\n",
    "      mlflow.log_artifact(temp_name, \"feature-importance.csv\")\n",
    "    finally:\n",
    "      temp.close() # Delete the temp file\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.residplot(predictions, y_test, lowess=True)\n",
    "    plt.xlabel(\"Predicted values for Price ($)\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.title(\"Residual Plot\")\n",
    "\n",
    "    # Log residuals using a temporary file\n",
    "    temp = tempfile.NamedTemporaryFile(prefix=\"residuals-\", suffix=\".png\")\n",
    "    temp_name = temp.name\n",
    "    try:\n",
    "      fig.savefig(temp_name)\n",
    "      mlflow.log_artifact(temp_name, \"residuals.png\")\n",
    "    finally:\n",
    "      temp.close() # Delete the temp file\n",
    "      \n",
    "    display(fig)\n",
    "    return run.info.run_uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with new parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  \"n_estimators\": 100,\n",
    "  \"max_depth\": 5,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Second Run\", params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the UI to see how this appears.  Take a look at the artifact to see where the plot was saved.\n",
    "\n",
    "Now, run a third run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1000_trees = {\n",
    "  \"n_estimators\": 1000,\n",
    "  \"max_depth\": 10,\n",
    "  \"random_state\": 42\n",
    "}\n",
    "\n",
    "log_rf(experimentID, \"Third Run\", params_1000_trees, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Past Runs\n",
    "\n",
    "You can query past runs programatically in order to use this data back in Python.  The pathway to doing this is an `MlflowClient` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from  mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now list all the runs for your experiment using `.list_run_infos()`, which takes your `experiment_id` as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[26]: [&lt;RunInfo: artifact_uri=&#39;dbfs:/databricks/mlflow/3227357976395900/5a9199cd9e8d4387b3345de0af61871e/artifacts&#39;, end_time=1587023519377, experiment_id=&#39;3227357976395900&#39;, lifecycle_stage=&#39;active&#39;, run_id=&#39;5a9199cd9e8d4387b3345de0af61871e&#39;, run_uuid=&#39;5a9199cd9e8d4387b3345de0af61871e&#39;, start_time=1587023490410, status=&#39;FINISHED&#39;, user_id=&#39;&#39;&gt;,\n",
       " &lt;RunInfo: artifact_uri=&#39;dbfs:/databricks/mlflow/3227357976395900/70204172cc4d4c59a38035f4e03cf96b/artifacts&#39;, end_time=1587023476035, experiment_id=&#39;3227357976395900&#39;, lifecycle_stage=&#39;active&#39;, run_id=&#39;70204172cc4d4c59a38035f4e03cf96b&#39;, run_uuid=&#39;70204172cc4d4c59a38035f4e03cf96b&#39;, start_time=1587023470895, status=&#39;FINISHED&#39;, user_id=&#39;&#39;&gt;,\n",
       " &lt;RunInfo: artifact_uri=&#39;dbfs:/databricks/mlflow/3227357976395900/aef808c7a24547e18e732577a4f03b51/artifacts&#39;, end_time=1587023278520, experiment_id=&#39;3227357976395900&#39;, lifecycle_stage=&#39;active&#39;, run_id=&#39;aef808c7a24547e18e732577a4f03b51&#39;, run_uuid=&#39;aef808c7a24547e18e732577a4f03b51&#39;, start_time=1587023274734, status=&#39;FINISHED&#39;, user_id=&#39;&#39;&gt;]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client.list_run_infos(experimentID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out a few fields and create a pandas DataFrame with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>run_uuid</th><th>start_time</th><th>artifact_uri</th></tr></thead><tbody><tr><td>5a9199cd9e8d4387b3345de0af61871e</td><td>1587023490410</td><td>dbfs:/databricks/mlflow/3227357976395900/5a9199cd9e8d4387b3345de0af61871e/artifacts</td></tr><tr><td>70204172cc4d4c59a38035f4e03cf96b</td><td>1587023470895</td><td>dbfs:/databricks/mlflow/3227357976395900/70204172cc4d4c59a38035f4e03cf96b/artifacts</td></tr><tr><td>aef808c7a24547e18e732577a4f03b51</td><td>1587023274734</td><td>dbfs:/databricks/mlflow/3227357976395900/aef808c7a24547e18e732577a4f03b51/artifacts</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runs = pd.DataFrame([(run.run_uuid, run.start_time, run.artifact_uri) for run in client.list_run_infos(experimentID)])\n",
    "runs.columns = [\"run_uuid\", \"start_time\", \"artifact_uri\"]\n",
    "\n",
    "display(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the last run and take a look at the associated artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[28]: [FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/5a9199cd9e8d4387b3345de0af61871e/artifacts/random-forest-model/MLmodel&#39;, name=&#39;MLmodel&#39;, size=357),\n",
       " FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/5a9199cd9e8d4387b3345de0af61871e/artifacts/random-forest-model/conda.yaml&#39;, name=&#39;conda.yaml&#39;, size=130),\n",
       " FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/5a9199cd9e8d4387b3345de0af61871e/artifacts/random-forest-model/model.pkl&#39;, name=&#39;model.pkl&#39;, size=46665877)]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "last_run = runs.sort_values(\"start_time\", ascending=False).iloc[0]\n",
    "\n",
    "dbutils.fs.ls(last_run[\"artifact_uri\"]+\"/random-forest-model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the evaluation metrics for the last run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[29]: {&#39;mae&#39;: 48.82679545743182, &#39;mse&#39;: 6013.141341974693, &#39;r2&#39;: 0.7063859458544959}</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client.get_run(last_run.run_uuid).data.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the model and take a look at the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[30]: [FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/5a9199cd9e8d4387b3345de0af61871e/artifacts/feature-importance.csv/&#39;, name=&#39;feature-importance.csv/&#39;, size=0),\n",
       " FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/5a9199cd9e8d4387b3345de0af61871e/artifacts/random-forest-model/&#39;, name=&#39;random-forest-model/&#39;, size=0),\n",
       " FileInfo(path=&#39;dbfs:/databricks/mlflow/3227357976395900/5a9199cd9e8d4387b3345de0af61871e/artifacts/residuals.png/&#39;, name=&#39;residuals.png/&#39;, size=0)]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.ls(last_run.artifact_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[31]: array([0.02066125, 0.01968234, 0.01714745, 0.10652883, 0.0466081 ,\n",
       "       0.0197694 , 0.04940956, 0.05043625, 0.09587796, 0.45534329,\n",
       "       0.01225107, 0.00644477, 0.01970861, 0.03186636, 0.0288097 ,\n",
       "       0.00256674, 0.00629972, 0.00134757, 0.0006453 , 0.00505837,\n",
       "       0.00353737])</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "model = mlflow.sklearn.load_model(last_run.artifact_uri + \"/random-forest-model/\")\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** What can MLflow Tracking log?  \n",
    "**Answer:** MLflow can log the following:\n",
    "- **Parameters:** inputs to a model\n",
    "- **Metrics:** the performance of the model\n",
    "- **Artifacts:** any object including data, models, and images\n",
    "- **Source:** the original code, including the commit hash if linked to git\n",
    "\n",
    "**Question:** How do you log experiments?  \n",
    "**Answer:** Experiments are logged by first creating a run and using the logging methods on that run object (e.g. `run.log_param(\"MSE\", .2)`).\n",
    "\n",
    "**Question:** Where do logged artifacts get saved?  \n",
    "**Answer:** Logged artifacts are saved in a directory of your choosing.  On Databricks, this would be DBFS, or the Databricks File System, which backed by a blob store.\n",
    "\n",
    "**Question:** How can I query past runs?  \n",
    "**Answer:** This can be done using an `MlflowClient` object.  This allows you do everything you can within the UI programatically so you never have to step outside of your programming environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Cleanup\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "02-Experiment-Tracking",
  "notebookId": 3227357976395900
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
