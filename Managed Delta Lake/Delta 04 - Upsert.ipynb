{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up relevant paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deltaMiniDataPath = workingDir + \"/customer-data-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPSERT \n",
    "\n",
    "Literally means \"UPdate\" and \"inSERT\". It means to atomically either insert a row, or, if the row already exists, UPDATE the row.\n",
    "\n",
    "It is also called **MERGE INTO**, which is what the Databricks Delta operation is called.  \n",
    "\n",
    "Alter the data by changing the values in one of the columns for a specific `CustomerID`.\n",
    "\n",
    "Let's load the CSV file `/mnt/training/online_retail/outdoor-products/outdoor-products-mini.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "miniDataInputPath = \"/mnt/training/online_retail/outdoor-products/outdoor-products-mini.csv\"\n",
    "inputSchema = \"InvoiceNo STRING, StockCode STRING, Description STRING, Quantity INT, InvoiceDate STRING, UnitPrice DOUBLE, CustomerID INT, Country STRING\"\n",
    "\n",
    "miniDataDF = (spark.read          \n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(inputSchema)\n",
    "  .csv(miniDataInputPath)                            \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPSERT Using Non-Databricks Delta Pipeline\n",
    "\n",
    "This feature is not supported in non-Delta pipelines.\n",
    "\n",
    "To UPSERT means to \"UPdate\" and \"inSERT\". In other words, UPSERT is not an atomic operation. It is literally TWO operations. \n",
    "\n",
    "Running an UPDATE could invalidate data that is accessed by the subsequent INSERT operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPSERT Using Databricks Delta Pipeline\n",
    "\n",
    "Using Databricks Delta, however, we can do UPSERTS.\n",
    "\n",
    "In this Lesson, we will explicitly create tables as SQL notation works better with UPSERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[16]: DataFrame[]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(miniDataDF\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .format(\"delta\")\n",
    "  .save(deltaMiniDataPath) \n",
    ")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {}.customer_data_delta_mini\n",
    "    USING DELTA \n",
    "    LOCATION '{}' \n",
    "  \"\"\".format(databaseName, deltaMiniDataPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all rows with `CustomerID=20993`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th><th>Quantity</th><th>InvoiceDate</th><th>UnitPrice</th><th>CustomerID</th><th>Country</th></tr></thead><tbody><tr><td>536371</td><td>32129</td><td>EverGlow Single</td><td>228</td><td>1/1/18 9:01</td><td>33.85</td><td>20993</td><td>Sierra Leone</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqlCmd = \"SELECT * FROM {}.customer_data_delta_mini WHERE CustomerID=20993\".format(databaseName)\n",
    "display(spark.sql(sqlCmd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form a new DataFrame where `StockCode` is `99999` for `CustomerID=20993`.\n",
    "\n",
    "Create a table `customer_data_delta_to_upsert` that contains this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "customerSpecificDF = (miniDataDF\n",
    "  .filter(\"CustomerID=20993\")\n",
    "  .withColumn(\"StockCode\", lit(99999))\n",
    " )\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS {}.customer_data_delta_to_upsert\".format(databaseName))\n",
    "customerSpecificDF.write.saveAsTable(\"{}.customer_data_delta_to_upsert\".format(databaseName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsert the new data into `customer_data_delta_mini`.\n",
    "\n",
    "Upsert is done using the `MERGE INTO` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[19]: DataFrame[]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"USE {}\".format(databaseName))\n",
    "\n",
    "sqlCmd = \"\"\"\n",
    "  MERGE INTO customer_data_delta_mini\n",
    "  USING customer_data_delta_to_upsert\n",
    "  ON customer_data_delta_mini.CustomerID = customer_data_delta_to_upsert.CustomerID\n",
    "  WHEN MATCHED THEN\n",
    "    UPDATE SET\n",
    "      customer_data_delta_mini.StockCode = customer_data_delta_to_upsert.StockCode\n",
    "  WHEN NOT MATCHED\n",
    "    THEN INSERT (InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country)\n",
    "    VALUES (\n",
    "      customer_data_delta_to_upsert.InvoiceNo,\n",
    "      customer_data_delta_to_upsert.StockCode, \n",
    "      customer_data_delta_to_upsert.Description, \n",
    "      customer_data_delta_to_upsert.Quantity, \n",
    "      customer_data_delta_to_upsert.InvoiceDate, \n",
    "      customer_data_delta_to_upsert.UnitPrice, \n",
    "      customer_data_delta_to_upsert.CustomerID, \n",
    "      customer_data_delta_to_upsert.Country)\"\"\"\n",
    "spark.sql(sqlCmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this data is seamlessly incorporated into `customer_data_delta_mini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>InvoiceNo</th><th>StockCode</th><th>Description</th><th>Quantity</th><th>InvoiceDate</th><th>UnitPrice</th><th>CustomerID</th><th>Country</th></tr></thead><tbody><tr><td>536371</td><td>99999</td><td>EverGlow Single</td><td>228</td><td>1/1/18 9:01</td><td>33.85</td><td>20993</td><td>Sierra Leone</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqlCmd = \"SELECT * FROM {}.customer_data_delta_mini WHERE CustomerID=20993\".format(databaseName)\n",
    "display(spark.sql(sqlCmd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Write base data to `deltaIotPath`.\n",
    "\n",
    "We do this for you, so just run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, from_unixtime, to_date\n",
    "jsonSchema = \"action string, time long\"\n",
    "streamingEventPath = \"/mnt/training/structured-streaming/events/\"\n",
    "deltaIotPath = workingDir + \"/iot-pipeline\"\n",
    "\n",
    "(spark.read \n",
    "  .schema(jsonSchema)\n",
    "  .json(streamingEventPath) \n",
    "  .withColumn(\"date\", to_date(from_unixtime(col(\"time\").cast(\"Long\"),\"yyyy-MM-dd\")))\n",
    "  .withColumn(\"deviceId\", expr(\"cast(rand(5) * 100 as int)\"))\n",
    "  .repartition(200)\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .format(\"delta\")\n",
    "  .partitionBy(\"date\")\n",
    "  .save(deltaIotPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Create a DataFrame out of the the data sitting in `deltaIotPath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "deltaIotPath = workingDir + \"/iot-pipeline\"\n",
    "\n",
    "newDataDF = spark.sql(\"SELECT * FROM delta.`{}` \".format(deltaIotPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Tests passed!\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST  - Run this cell to test your solution.\n",
    "schema = str(newDataDF.schema)\n",
    "\n",
    "dbTest(\"assert-1\", True, \"action,StringType\" in schema)\n",
    "dbTest(\"assert-2\", True, \"time,LongType\" in schema)\n",
    "dbTest(\"assert-3\", True, \"date,DateType\" in schema)\n",
    "dbTest(\"assert-4\", True, \"deviceId,IntegerType\" in schema)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Create another DataFrame `newDeviceIdDF`\n",
    "* Pick up the 1st row you see that has `action` set to `Open`.\n",
    "\n",
    "* Change `action` to `Close`.\n",
    "\n",
    "* We will use the associated `deviceId` in the cells that follow.\n",
    "* The DataFrame you construct should only have 1 row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "devId = (newDataDF\n",
    "  .select(\"deviceId\")\n",
    "  .filter(col(\"action\") == \"Open\")\n",
    "  .limit(1)\n",
    "  .first()[0])\n",
    "  \n",
    "newDeviceIdDF = (newDataDF\n",
    "  .filter(col(\"deviceId\") == devId)\n",
    "  .withColumn(\"action\", lit(\"Close\")) \n",
    "  .limit(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Tests passed!\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "actionCount = newDeviceIdDF.filter(col(\"Action\") == \"Close\").count()\n",
    "\n",
    "dbTest(\"Delta-L4-actionCount\", 1, actionCount)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Write to a new Databricks Delta table named `iot_data_delta_to_upsert` that contains just our data to be upserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "spark.sql(\"DROP TABLE IF EXISTS {}.iot_data_delta_to_upsert\".format(databaseName))\n",
    "newDeviceIdDF.write.saveAsTable(\"{}.iot_data_delta_to_upsert\".format(databaseName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Tests passed!\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "count = spark.table(\"{}.iot_data_delta_to_upsert\".format(databaseName)).count()\n",
    "\n",
    "dbTest(\"Delta-04-demoIotTableHasRow\", True, count > 0)  \n",
    "  \n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Create a Databricks Delta table named `demo_iot_data_delta` that contains just the data from `deltaIotPath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[28]: DataFrame[]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "sqlCmd = \"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS {}.demo_iot_data_delta\n",
    "  USING DELTA\n",
    "  LOCATION '{}'\"\"\".format(databaseName, deltaIotPath)\n",
    "\n",
    "spark.sql(sqlCmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Tests passed!\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "try:\n",
    "  tableExists = (spark.table(\"{}.demo_iot_data_delta\".format(databaseName)).count() > 0)\n",
    "except:\n",
    "  tableExists = False\n",
    "  \n",
    "dbTest(\"Delta-04-demoTableExists\", True, tableExists)  \n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "Insert the data `iot_data_delta_to_upsert` into `demo_iot_data_delta`.\n",
    "\n",
    "You can adapt the SQL syntax for the upsert from our demo example, above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[30]: DataFrame[]</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "spark.sql(\"USE {}\".format(databaseName))\n",
    "\n",
    "sqlCmd = \"\"\"\n",
    "  MERGE INTO demo_iot_data_delta\n",
    "  USING iot_data_delta_to_upsert\n",
    "  ON demo_iot_data_delta.deviceId = iot_data_delta_to_upsert.deviceId\n",
    "  WHEN MATCHED THEN\n",
    "    UPDATE SET\n",
    "      demo_iot_data_delta.action = iot_data_delta_to_upsert.action\n",
    "  WHEN NOT MATCHED\n",
    "    THEN INSERT (action, time, date, deviceId)\n",
    "    VALUES (\n",
    "      iot_data_delta_to_upsert.action, \n",
    "      iot_data_delta_to_upsert.time, \n",
    "      iot_data_delta_to_upsert.date, \n",
    "      iot_data_delta_to_upsert.deviceId \n",
    "  )\"\"\"\n",
    "\n",
    "spark.sql(sqlCmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "devId = newDeviceIdDF.select(\"deviceId\").first()[0]\n",
    "\n",
    "sqlCmd1 = \"SELECT count(*) as total FROM {}.demo_iot_data_delta WHERE deviceId = {} AND action = 'Open' \".format(databaseName, devId)\n",
    "countOpen = spark.sql(sqlCmd1).first()[0]\n",
    "\n",
    "sqlCmd2 = \"SELECT count(*) as total FROM {}.demo_iot_data_delta WHERE deviceId = {} AND action = 'Close' \".format(databaseName, devId)\n",
    "countClose = spark.sql(sqlCmd2).first()[0]\n",
    "\n",
    "dbTest(\"Delta-L4-count\", True, countOpen == 0 and countClose > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "Count the number of items in `demo_iot_data_delta` where \n",
    "* `deviceId` is obtained from this query `newDeviceIdDF.select(\"deviceId\").first()[0]` .\n",
    "* `action` is `Close`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "sqlCmd = \"SELECT count(*) as total FROM {}.demo_iot_data_delta WHERE deviceId = {} AND action = 'Close' \".format(databaseName, devId)\n",
    "count = spark.sql(sqlCmd).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Tests passed!\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST - Run this cell to test your solution.\n",
    "dbTest(\"Delta-L4-demoiot-count\", True, count > 0)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Cleanup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "**Q:** What does it mean to UPSERT?<br>\n",
    "**A:** To UPSERT is to either INSERT a row, or if the row already exists, UPDATE the row.\n",
    "\n",
    "**Q:** What happens if you try to UPSERT in a parquet-based data set?<br>\n",
    "**A:** That's not possible due to the schema-on-read paradigm, you will get an error until you repair the table.\n",
    "\n",
    "**Q:** How to you perform UPSERT in a Databricks Delta dataset?<br>\n",
    "**A:** Using the `MERGE INTO my-table USING data-to-upsert`.\n",
    "\n",
    "**Q:** What is the caveat to `USING data-to-upsert`?<br>\n",
    "**A:** Your source data has ALL the data you want to replace: in other words, you create a new dataframe that has the source data you want to replace in the Databricks Delta table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Delta 04 - Upsert",
  "notebookId": 3227357976395076
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
