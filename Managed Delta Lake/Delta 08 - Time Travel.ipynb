{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Delta Time Travel\n",
    "\n",
    "The Databricks Delta log has a list of what files are valid for each read / write operation.\n",
    "\n",
    "By referencing this list, a request can be made for the data at a specific point in time. \n",
    "\n",
    "This is similar to the concept of code Revision histories.\n",
    "\n",
    "Examples of Time Travel use cases are:\n",
    "* Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). \n",
    "  * This could be useful for debugging or auditing, especially in regulated industries.\n",
    "* Writing complex temporal queries.\n",
    "* Fixing mistakes in your data.\n",
    "* Providing snapshot isolation for a set of queries for fast changing tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slow Stream of Files\n",
    "\n",
    "Our stream source is a repository of many small files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "dataPath = \"/mnt/training/power-plant/streamed.parquet\"\n",
    "\n",
    "dataSchema = StructType([\n",
    "  StructField(\"AT\", DoubleType(), True),\n",
    "  StructField(\"V\", DoubleType(), True),\n",
    "  StructField(\"AP\", DoubleType(), True),\n",
    "  StructField(\"RH\", DoubleType(), True),\n",
    "  StructField(\"PE\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "initialDF = (spark\n",
    "  .readStream                        # Returns DataStreamReader\n",
    "  .option(\"maxFilesPerTrigger\", 1)   # Force processing of only 1 file per trigger \n",
    "  .schema(dataSchema)                # Required for all streaming DataFrames\n",
    "  .parquet(dataPath) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append to a Databricks Delta Table\n",
    "\n",
    "Use this to create `powerTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "writePath      = workingDir + \"/output.parquet\"    # A subdirectory for our output\n",
    "checkpointPath = workingDir + \"/output.checkpoint\" # A subdirectory for our checkpoint & W-A logs\n",
    "\n",
    "powerTable = \"powerTable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to help us manage our streams better, we will make use of **`untilStreamIsReady()`**, **`stopAllStreams()`** and define the following, **`myStreamName`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myStreamName = \"lesson08_ps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Introducing Time Travel\n",
    "\n",
    "Databricks Delta time travel allows you to query an older snapshot of a table.\n",
    "\n",
    "Here, we introduce a new option to Databricks Delta.\n",
    "\n",
    "`.option(\"timestampAsOf\", now)` \n",
    "\n",
    "Where `now` is the current timestamp, that must be a STRING that can be cast to a Timestamp.\n",
    "\n",
    "There is an alternate notation as well \n",
    "\n",
    "`.option(\"versionAsOf\", version)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "streamingQuery = (initialDF                     # Start with our \"streaming\" DataFrame\n",
    "  .writeStream                                  # Get the DataStreamWriter\n",
    "  .trigger(processingTime=\"3 seconds\")          # Configure for a 3-second micro-batch\n",
    "  .queryName(myStreamName)                       # Specify Query Name\n",
    "  .format(\"delta\")                              # Specify the sink type, a Parquet file\n",
    "  .option(\"timestampAsOf\", now)                 # Timestamp the stream in the form of string that can be converted to TimeStamp\n",
    "  .outputMode(\"append\")                         # Write only new data to the \"file\"\n",
    "  .option(\"checkpointLocation\", checkpointPath) # Specify the location of checkpoint files & W-A logs\n",
    "  .table(powerTable)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">The stream lesson08_ps is active and ready.\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wait until the stream is ready before proceeding\n",
    "untilStreamIsReady(myStreamName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retention Period and Table Properties\n",
    "\n",
    "You configure retention periods using `ALTER TABLE` syntax with the following table properties:\n",
    "\n",
    "* `delta.logRetentionDuration \"interval interval-string\" `\n",
    "  * Configure how long you can go back in time. Default is interval 30 days.\n",
    "\n",
    "* `delta.deletedFileRetentionDuration = \"interval interval-string\" `\n",
    "  * Configure how long stale data files are kept around before being deleted with VACUUM. Default is interval 1 week.\n",
    "  \n",
    "* `interval-string` is in the form `30 days` or `1 week`\n",
    "\n",
    "For full access to 30 days of historical data, set `delta.deletedFileRetentionDuration = \"interval 30 days\" ` on your table. \n",
    "\n",
    "Using a large number of days may cause your storage costs to go way up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>delta.deletedFileRetentionDuration</td><td>interval 10 days</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"ALTER TABLE {powerTable} SET TBLPROPERTIES (delta.deletedFileRetentionDuration=\"interval 10 days\") \"\"\")\n",
    "tblPropDF = spark.sql(f\"SHOW TBLPROPERTIES {powerTable}\")\n",
    "display(tblPropDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell multiple times to show that the data is changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>144</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "countDF = spark.sql(f\"SELECT count(*) FROM {powerTable}\")\n",
    "display(countDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th></tr></thead><tbody><tr><td>2020-04-15T11:05:55.000+0000</td></tr><tr><td>2020-04-15T11:06:15.000+0000</td></tr><tr><td>2020-04-15T11:06:36.000+0000</td></tr><tr><td>2020-04-15T11:07:03.000+0000</td></tr><tr><td>2020-04-15T11:07:34.000+0000</td></tr><tr><td>2020-04-15T11:08:13.000+0000</td></tr><tr><td>2020-04-15T11:08:58.000+0000</td></tr><tr><td>2020-04-15T11:09:52.000+0000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "historyDF = spark.sql(f\"SELECT timestamp FROM (DESCRIBE HISTORY {powerTable}) ORDER BY timestamp\")\n",
    "display(historyDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewind back to almost the beginning (where we had just a handful of rows), let's say the 2nd write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List timestamps of when table writes occurred\n",
    "historyDF = spark.sql(f\"SELECT timestamp FROM (DESCRIBE HISTORY {powerTable}) ORDER BY timestamp\")\n",
    "\n",
    "# Pick out 2nd write\n",
    "oldTimestamp = historyDF.take(2)[-1].timestamp\n",
    "\n",
    "# Re-build the DataFrame as it was in the 2nd write\n",
    "rewoundDF = spark.sql(f\"SELECT * FROM {powerTable} TIMESTAMP AS OF '{oldTimestamp}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had this many (few) rows back then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[22]: 24</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewoundDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop all remaining streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stopAllStreams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Cleanup\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Delta 08 - Time Travel",
  "notebookId": 3227357976395262
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
