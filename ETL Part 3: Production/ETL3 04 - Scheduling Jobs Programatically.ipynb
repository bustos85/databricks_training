{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating ETL Workloads\n",
    "\n",
    "Since recurring production jobs are the goal of ETL workloads, Spark needs a way to integrate with other automation and scheduling tools.  We also need to be able to run Python files and Scala/Java jars.\n",
    "\n",
    "There are a number of different automation and scheduling tools including the following:<br><br>\n",
    "\n",
    "* Command line tools integrated with the UNIX scheduler Cron\n",
    "* The workflow scheduler Apache Airflow\n",
    "* Amazon's trigger-based scheduler AWS Lambda\n",
    "\n",
    "The gateway into job scheduling is programmatic access to Databricks, which can be achieved either through the REST API or the Databricks Command Line Interface (CLI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Tokens\n",
    "\n",
    "Access tokens provide programmatic access to the Databricks CLI and REST API.  \n",
    "\n",
    "To get started, first generate an access token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate a token:<br><br>\n",
    "\n",
    "1. Click on the person icon in the upper-right corner of the screen.\n",
    "2. Click **User Settings**\n",
    "3. Click on **Access Tokens**\n",
    "4. Click on **Generate New Token**\n",
    "5. Name your token\n",
    "6. Designate a lifespan (a shorter lifespan is generally better to minimize risk exposure)\n",
    "7. Click **Generate**\n",
    "8. Copy your token.  You'll only be able to see it once.\n",
    "\n",
    "Be sure to keep this key secure.  This grants the holder full programmatic access to Databricks, including both resources and data that's available to your Databricks environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste your token into the following cell along with the domain of your Databricks deployment (you can see this in the notebook's URL).  The deployment should look something like `https://my-deployment.cloud.databricks.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "token = \"FILL IN\"\n",
    "domain = \"https://<DOMAIN NAME>.cloud.databricks.com\" + \"/api/2.0/\"\n",
    "\n",
    "header = {'Authorization': \"Bearer \"+ token}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the connection works by listing all files in the root directory of DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import json\n",
    "  import requests\n",
    "\n",
    "  endPoint = domain+\"dbfs/list?path=/\"\n",
    "  r = requests.get(endPoint, headers=header)\n",
    "\n",
    "  [i.get(\"path\") for i in json.loads(r.text).get(\"files\")]  \n",
    "\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "  print(\"\\n** Double check your previous settings **\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling with the REST API and CLI\n",
    "\n",
    "Jobs can either be scheduled for running on a consistent basis or they can be run every time the API call is made.  Since there are many parameters in scheduling jobs, it's often best to schedule a job through the user interface, parse the configuration settings, and then run later jobs using the API.\n",
    "\n",
    "Run the following cell to get the sense of what a basic job accomplishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dbutils.notebook.run(\"./Runnable/Runnable-4\", 120, {\"username\": getUsername(), \"ranBy\": \"NOTEBOOK\"})\n",
    "display(spark.read.parquet(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook `Runnable-4` logs a timestamp and how the notebook is run.  This will log our jobs.\n",
    "\n",
    "Schedule this job notebook as a job using parameters by first navigating to the jobs panel on the left-hand side of the screen and creating a new job.  Customize the job as follows:<br><br>\n",
    "\n",
    "1. Give the job a name\n",
    "2. Choose the notebook `Runnable-4` in the `Runnable` directory of this course\n",
    "3. Add parameters for `username`, which is your Databricks login email (this gives you a unique path to save your data), and set `ranBy` as `JOB`\n",
    "4. Choose a cluster of 2 workers and 1 driver (the default is too large for our needs).  **You can also choose to run a job against an already active cluster, reducing the time to spin up new resources.**\n",
    "5. Click **Run now** to execute the job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the job completes, paste the `Run ID` that appears under completed runs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  runId = \"FILL_IN\"\n",
    "  endPoint = domain + \"jobs/runs/get?run_id={}\".format(runId)\n",
    "\n",
    "  json.loads(requests.get(endPoint, headers=header).text)\n",
    "  \n",
    "except Exception as e:\n",
    "  print(e)\n",
    "  print(\"\\n** Double check your runId and domain **\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the table to see the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.parquet(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Create and Submit a Job using the REST API\n",
    "\n",
    "Now that a job has been submitted through the UI, we can easily capture and re-run that job.  Re-run the job using the REST API and different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the `POST` Request Payload\n",
    "\n",
    "To create a new job, communicate the specifications about the job using a `POST` request.  First, define the following variables:<br><br>\n",
    "\n",
    "* `name`: The name of your job\n",
    "* `notebook_path`: The path to the notebook `Runnable-4`.  This will be the `noteboook_path` variable listed in the API call above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{&#34;name&#34;: &#34;Lesson-04-Lab&#34;, &#34;new_cluster&#34;: {&#34;spark_version&#34;: &#34;4.2.x-scala2.11&#34;, &#34;node_type_id&#34;: &#34;i3.xlarge&#34;, &#34;num_workers&#34;: 2, &#34;spark_conf&#34;: {&#34;spark.databricks.delta.preview.enabled&#34;: &#34;true&#34;}}, &#34;notebook_task&#34;: {&#34;notebook_path&#34;: &#34;/Shared/ETL-Part-3/Python/Runnable/Runnable-4&#34;, &#34;base_parameters&#34;: {&#34;username&#34;: &#34;jose.manuel.bustos.munoz@everis.com&#34;, &#34;ranBy&#34;: &#34;REST-API&#34;}}}\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "import json\n",
    "\n",
    "name = \"Lesson-04-Lab\"\n",
    "notebook_path = \"/Shared/ETL-Part-3/Python/Runnable/Runnable-4\"\n",
    "\n",
    "data = {\n",
    "  \"name\": name,\n",
    "  \"new_cluster\": {\n",
    "    \"spark_version\": \"4.2.x-scala2.11\",\n",
    "    \"node_type_id\": \"i3.xlarge\",\n",
    "    \"num_workers\": 2,\n",
    "    \"spark_conf\": {\"spark.databricks.delta.preview.enabled\": \"true\"}\n",
    "  },\n",
    "  \"notebook_task\": {\n",
    "    \"notebook_path\": notebook_path,\n",
    "    \"base_parameters\": {\n",
    "      \"username\": username, \"ranBy\": \"REST-API\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "data_str = json.dumps(data)\n",
    "print(data_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Job\n",
    "\n",
    "Use the base `domain` defined above to create a URL for the REST endpoint `jobs/create`.  Then, submit a `POST` request using `data_str` as the payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "createEndPoint = domain + \"jobs/create\"\n",
    "r = requests.post(createEndPoint, headers=header, data=data_str)\n",
    "\n",
    "job_id = json.loads(r.text).get(\"job_id\")\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the Job\n",
    "\n",
    "Run the job using the `job_id` from above.  You'll need to submit the post request to the `RunEndPoint` URL of `jobs/run-now`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "RunEndPoint = domain + \"jobs/run-now\"\n",
    "\n",
    "data2 = {\"job_id\": job_id}\n",
    "data2_str = json.dumps(data2)\n",
    "\n",
    "r = requests.post(RunEndPoint, headers=header, data=data2_str)\n",
    "\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Confirm that the Job Ran\n",
    "\n",
    "Confirm that the job ran by checking the parquet file.  It can take a few minutes for the job to run and update this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.parquet(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "APICounts = (spark.read.parquet(path)\n",
    "  .filter(col(\"ranBy\") == \"REST-API\")\n",
    "  .count()\n",
    ")\n",
    "\n",
    "if APICounts > 0:\n",
    "  print(\"Tests passed!\")\n",
    "else:\n",
    "  print(\"Test failed, no records found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "**Question:** What ways can you schedule jobs on Databricks?  \n",
    "**Answer:** Jobs can be scheduled using the UI, REST API, or Databricks CLI.\n",
    "\n",
    "**Question:** How can you gain programmatic access to Databricks?  \n",
    "**Answer:** Generating a token will give programmatic access to most Databricks services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Cleanup\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "ETL3 04 - Scheduling Jobs Programatically",
  "notebookId": 318591916144152
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
